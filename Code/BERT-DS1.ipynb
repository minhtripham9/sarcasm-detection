{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## BERT-DS1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "54136140"
   },
   "source": [
    "### Managing imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "id": "556eb40f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "afc5aaa3"
   },
   "source": [
    "## Without context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "id": "f2b5f40b"
   },
   "source": [
    "#### Loading dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "id": "c21bd9f3",
    "outputId": "be52598c-e230-4bce-9349-762a1970669a"
   },
   "outputs": [],
   "source": [
    "# Load the JSONL file (lines=True is key!)\n",
    "df1 = pd.read_json('../Dataset/sarcasm_detection_shared_task_reddit_training.jsonl', lines=True)\n",
    "\n",
    "# Map label: SARCASM -> 1, NOT_SARCASM -> 0\n",
    "df1['label'] = df1['label'].replace({'SARCASM': 1, 'NOT_SARCASM': 0}).astype(int)\n",
    "\n",
    "def get_first_context(context):\n",
    "    if isinstance(context, list) and context:\n",
    "        return context[0]\n",
    "    return context  # in case it's empty or not a list\n",
    "\n",
    "df1['context'] = df1['context'].apply(get_first_context)\n",
    "df1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "id": "5ba827db"
   },
   "source": [
    "#### Dataset 1 preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "560c9ec1",
    "outputId": "953b5200-c752-490c-fcce-2f94b1b335f7"
   },
   "outputs": [],
   "source": [
    "df1['input_text'] = df1['response']\n",
    "\n",
    "# Check the result\n",
    "print(df1[['input_text', 'label']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YiwGiYZNHGlK",
    "outputId": "2e80d782-4c20-482a-c628-eda3973b856b"
   },
   "outputs": [],
   "source": [
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df1['input_text'], df1['label'],\n",
    "    test_size=0.2, random_state=42, stratify=df1['label']\n",
    ")\n",
    "\n",
    "train_df1 = pd.DataFrame({'input_text': X_train, 'label': y_train})\n",
    "test_df1 = pd.DataFrame({'input_text': X_test, 'label': y_test})\n",
    "\n",
    "\n",
    "print(\"Train and test splits saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "id": "f726fd50"
   },
   "source": [
    "### Transformers based models : RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "id": "af3d6cbc"
   },
   "source": [
    "#### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301,
     "referenced_widgets": [
      "7529f333a3b04fbea322de1e6c0d5fff",
      "4dbbbd81095142d49bfdc8508ee35f90",
      "ac70df3fd5d04c94910043635cc2747b",
      "4152d176a45e4a518e5e84a53c067a20",
      "e95dc9afc3934aefac3c8a443a69d85e",
      "bfbc120f01e3421382fb9511d6f6a138",
      "58619695975a4f439b2e16600f67c0d3",
      "429760e8c4dd49108b714f6f931ca464",
      "a2626d35a4944763ab0eaac2910a3a84",
      "c6af50b4a8a043a7ab32883cd1b21911",
      "7900c6a075554d6081dad3e1840a3545",
      "916c0204e8084f90acdeb831f7d5a750",
      "351cf87d1e15429fab6e18815af3b110",
      "83bb590243f449dc8ba801aff75f0079",
      "92ecfe47f7694600956eab893c4f3d13",
      "9014a21b39c142b78be02886f6214ef2",
      "a2204a897a014a7d941d616af9f0097d",
      "30a2ef4f11d3423180f5695697b8de92",
      "fd993cefd4ac48469d5159dc21bb9f1d",
      "3ce6ffe7ebc84980b78682cfaa82dc17",
      "423b156ab535470eaa6c93a12aa458c6",
      "0bfa82c485904a9e90e50a4c341aa70a",
      "565e145e2936441caaef0e3119bcb385",
      "f2c2dc312ca243e8b7412e641a47f837",
      "57bd300135534e8dbb64c5868756538f",
      "b25004f315d94b399c352aacb1aefaa4",
      "4de5cc94219c463dbe2202b35f510ec9",
      "29dd8358de764780ab2c3898d018565d",
      "393c1c5f37174c54af9d2ecf3e4f4e36",
      "ce3ac75f94b4447dbdc9c34efb1e6905",
      "d8fd6d49f1f845d1b42a794fca43171b",
      "c22ea5250fe0442e9f598503087f068c",
      "07305ccbbabc42f08ae353f15f506fc4",
      "79d568ab971b48e1958884652d50bf1d",
      "22b8d87b85f5402991c9a4e778023389",
      "4b4c08f9bdd3480fa4ea2ae18856c46d",
      "0cc0299688584a1ba40cb96622b3b830",
      "df6711f6e2f842928aa2d06a2b907820",
      "9b71dbcd65804a1393d382a835d8d2be",
      "2c54947b9b124734ae1db67091efac98",
      "5e87cfc4379440dba721cce52bec4db5",
      "dc8bb41820234fd9984a913528fb5a62",
      "d3b2b67dbd734c0997de2676c0f84910",
      "7c94b13368474341acde9a793e828d3c",
      "9decd3ced0f140868f97f3b02b0220ee",
      "eb56343d12844625b28d707b0b6d3aa1",
      "e65082f203c64873b220d72630f88281",
      "11dc6785470d46d99b87b76b1a81e9a4",
      "9e6bde340b0b445ea82c2aea9bf3cc39",
      "e2ba52615bda4f38b63ef22f65504ce2",
      "02d18e813d314a269ce32fdd498e1e5d",
      "af4147bbba604c099575aca4bf7dc035",
      "7a28b0dbe7584091a1fe6b26b963ce99",
      "4399907c4bcb4f8f9b2f6b2b51fb00d4",
      "ea86c44aff4244249f5f6a25a68f6eb8"
     ]
    },
    "id": "a25c4576",
    "outputId": "024e2246-105b-4d2d-8dce-a57a2e73a86b"
   },
   "outputs": [],
   "source": [
    "X_train_roberta = train_df1['input_text'].astype(str).tolist()\n",
    "y_train_roberta = train_df1['label'].tolist()\n",
    "X_test_roberta = test_df1['input_text'].astype(str).tolist()\n",
    "y_test_roberta = test_df1['label'].tolist()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "id": "83ea4094"
   },
   "outputs": [],
   "source": [
    "X_train_enc_roberta = tokenizer(\n",
    "    X_train_roberta,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=110,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "X_test_enc_roberta = tokenizer(\n",
    "    X_test_roberta,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=110,\n",
    "    return_tensors='pt'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "id": "ddb92df9"
   },
   "outputs": [],
   "source": [
    "# Convert to torch tensors\n",
    "train_dataset_roberta = TensorDataset(\n",
    "    X_train_enc_roberta['input_ids'],\n",
    "    X_train_enc_roberta['attention_mask'],\n",
    "    torch.tensor(y_train_roberta)\n",
    ")\n",
    "\n",
    "test_dataset_roberta = TensorDataset(\n",
    "    X_test_enc_roberta['input_ids'],\n",
    "    X_test_enc_roberta['attention_mask'],\n",
    "    torch.tensor(y_test_roberta)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "id": "489a36bf"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset_roberta, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset_roberta, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "0c3c2ba7ee3744a7913099f967bd3875",
      "ffc02c5fdba94a49b2a80636cea6bea6",
      "838430bfb11c4355b64cdab04cc86812",
      "ed8e2e06af5941b6a6a0be7ab471ac24",
      "f5785cbc895a4c82b4b5314fc41a7f89",
      "bf2a1b447b934b428d1ad0065ff9931d",
      "1aa925604b0645a48cae7a7e52cab9e4",
      "e2dfeabfbc0e4baf8d55f5ca2e947a4f",
      "d6503b55d8584498bb96c37662a5a2ef",
      "f4fe8315355d4996b8fef7c0518ae44f",
      "b6d72e720ee147acbf0c55d82f1da9f9"
     ]
    },
    "id": "e78479c9",
    "outputId": "f77d851d-311c-4b8c-c179-31b430beeda5"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "model = model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "17075b3c"
   },
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1e38b0b6",
    "outputId": "2855474c-8c8e-4e0d-bc51-40eac3e4e61f"
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        b_input_ids, b_attn_mask, b_labels = [x.to(device) for x in batch]\n",
    "        outputs = model(input_ids=b_input_ids, attention_mask=b_attn_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "562e20f1"
   },
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "id": "92dde7cb",
    "outputId": "e122b168-e5b9-449e-c951-9466b9909c57"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "preds_roberta, truths_roberta = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        b_input_ids, b_attn_mask, b_labels = [x.to(device) for x in batch]\n",
    "        outputs = model(input_ids=b_input_ids, attention_mask=b_attn_mask)\n",
    "        logits = outputs.logits\n",
    "        preds_roberta.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "        truths_roberta.extend(b_labels.cpu().numpy())\n",
    "\n",
    "print(classification_report(truths_roberta, preds_roberta, target_names=['Not Sarcastic', 'Sarcastic']))\n",
    "\n",
    "cm_roberta = confusion_matrix(truths_roberta, preds_roberta)\n",
    "labels = ['Not Sarcastic', 'Sarcastic']\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_roberta, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix - RoBERTa Sarcasm Detection')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "id": "dc6fe394"
   },
   "source": [
    "## With context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "id": "19c2daf0"
   },
   "source": [
    "#### Dataset 1 preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9e2ef07",
    "outputId": "7dd721b0-06e0-4188-e446-5fafdc574672"
   },
   "outputs": [],
   "source": [
    "df1['input_text'] = df1['response'] + ' [SEP] ' + 'Context :' + df1['context']\n",
    "\n",
    "# Check the result\n",
    "print(df1[['input_text', 'label']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12e7bccc",
    "outputId": "6054195a-bade-4873-87f4-4c71be20f088"
   },
   "outputs": [],
   "source": [
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df1['input_text'], df1['label'],\n",
    "    test_size=0.2, random_state=42, stratify=df1['label']\n",
    ")\n",
    "\n",
    "train_df1 = pd.DataFrame({'input_text': X_train, 'label': y_train})\n",
    "test_df1 = pd.DataFrame({'input_text': X_test, 'label': y_test})\n",
    "\n",
    "print(\"Train and test splits saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "id": "6f3b5b95"
   },
   "source": [
    "### Transformers based models : RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "id": "b39a5a39"
   },
   "source": [
    "#### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "id": "5b1586b9"
   },
   "outputs": [],
   "source": [
    "X_train_roberta = train_df1['input_text'].astype(str).tolist()\n",
    "y_train_roberta = train_df1['label'].tolist()\n",
    "X_test_roberta = test_df1['input_text'].astype(str).tolist()\n",
    "y_test_roberta = test_df1['label'].tolist()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "id": "fc506299"
   },
   "outputs": [],
   "source": [
    "X_train_enc_roberta = tokenizer(\n",
    "    X_train_roberta,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=110,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "X_test_enc_roberta = tokenizer(\n",
    "    X_test_roberta,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=110,\n",
    "    return_tensors='pt'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "id": "522714e0"
   },
   "outputs": [],
   "source": [
    "# Convert to torch tensors\n",
    "train_dataset_roberta = TensorDataset(\n",
    "    X_train_enc_roberta['input_ids'],\n",
    "    X_train_enc_roberta['attention_mask'],\n",
    "    torch.tensor(y_train_roberta)\n",
    ")\n",
    "\n",
    "test_dataset_roberta = TensorDataset(\n",
    "    X_test_enc_roberta['input_ids'],\n",
    "    X_test_enc_roberta['attention_mask'],\n",
    "    torch.tensor(y_test_roberta)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "id": "7a5d134a"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset_roberta, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset_roberta, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e025e56",
    "outputId": "b876cf05-310c-47b0-a4d0-4327e6ca6d35"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "model = model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "id": "196e0695"
   },
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6a10dae3",
    "outputId": "0d228b8c-58a3-49c4-8472-94b750c124e4"
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        b_input_ids, b_attn_mask, b_labels = [x.to(device) for x in batch]\n",
    "        outputs = model(input_ids=b_input_ids, attention_mask=b_attn_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {
    "id": "e0d66eac"
   },
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "id": "fdfdb80c",
    "outputId": "9b1ca0dc-1589-4d6e-8c07-db898908a4f3"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "preds_roberta, truths_roberta = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        b_input_ids, b_attn_mask, b_labels = [x.to(device) for x in batch]\n",
    "        outputs = model(input_ids=b_input_ids, attention_mask=b_attn_mask)\n",
    "        logits = outputs.logits\n",
    "        preds_roberta.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "        truths_roberta.extend(b_labels.cpu().numpy())\n",
    "\n",
    "print(classification_report(truths_roberta, preds_roberta, target_names=['Not Sarcastic', 'Sarcastic']))\n",
    "\n",
    "cm_roberta = confusion_matrix(truths_roberta, preds_roberta)\n",
    "labels = ['Not Sarcastic', 'Sarcastic']\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_roberta, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix - RoBERTa Sarcasm Detection')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
